{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    print('foo')\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of courtesy comments: 10\n",
      "# of effectiveness comments: 17\n",
      "# of timeliness comments: 24\n",
      "# of understanding comments: 13\n",
      "# of nps comments: 148\n",
      "# of comments: 212\n"
     ]
    }
   ],
   "source": [
    "#TESTING DATA EXTRACTION\n",
    "import pandas\n",
    "#pandas.read_excel(r'C:\\Users\\n1555085\\Downloads\\Copy of CSAT Help hub April Responses.xlsx')\n",
    "sheet = pandas.read_excel(r'C:\\Users\\n1555085\\Downloads\\May_Help_Hub_Data.xlsx')\n",
    "courtesy = sheet['Unnamed: 9'].dropna().values.tolist()[1:]\n",
    "effectiveness = sheet['Unnamed: 10'].dropna().values.tolist()[1:]\n",
    "timeliness = sheet['Unnamed: 11'].dropna().values.tolist()[1:]\n",
    "understanding = sheet['Unnamed: 12'].dropna().values.tolist()[1:]\n",
    "nps = sheet['Unnamed: 13'].dropna().values.tolist()[1:]\n",
    "comments = courtesy + effectiveness + timeliness + understanding + nps\n",
    "date = sheet['Unnamed: 1'].dropna().values.tolist()[1:]\n",
    "#should add completion rate\n",
    "\n",
    "print(f'# of courtesy comments: {len(courtesy)}')\n",
    "print(f'# of effectiveness comments: {len(effectiveness)}')\n",
    "print(f'# of timeliness comments: {len(timeliness)}')\n",
    "print(f'# of understanding comments: {len(understanding)}')\n",
    "print(f'# of nps comments: {len(nps)}')\n",
    "print(f'# of comments: {len(comments)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4553 1574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Spenser Cameron set assisted issues. exceptional trying assist newcomer new systems.',\n",
       " 'second consultant awesome appropriate follow well letting know break fix consultants suggestion since could not receive emails. best.',\n",
       " 'support personnel helpful patient issue',\n",
       " 'fast service.',\n",
       " 'Greeting robotic, however loosened conversation went on.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING DATA PREPARATION\n",
    "\n",
    "#read from corpus, remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "with open(r'C:\\Users\\n1555085\\Downloads\\Project\\positiveComments.txt', 'r') as f:\n",
    "    posReviews = f.readlines()\n",
    "with open(r'C:\\Users\\n1555085\\Downloads\\Project\\negativeComments.txt', 'r') as f:\n",
    "    negReviews = f.readlines()\n",
    "print(len(posReviews), len(negReviews))\n",
    "\n",
    "sw = set(stopwords.words('english') + list(punctuation))\n",
    "notStopwords = ['not', 'no', '!', 'but', 'too', 'have', 'had']\n",
    "\n",
    "def removeStopwords(review):\n",
    "    #review.translate(None, string.punctuation)\n",
    "    return ' '.join([word for word in review.split() if word.lower() not in sw or word.lower() in notStopwords])\n",
    "posReviews = list(filter(lambda s: s , list(map(removeStopwords, posReviews))))\n",
    "negReviews = list(filter(lambda s: s , list(map(removeStopwords, negReviews))))\n",
    "\n",
    "posReviews[0:5]\n",
    "#negReviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define pos and neg bag-of words, and vocabulary\n",
    "\n",
    "posWords = [word.lower() for review in posReviews for word in review.split()]\n",
    "negWords = [word.lower() for review in negReviews for word in review.split()]\n",
    "vocabulary = list(set(posWords + negWords))\n",
    "#vocabulary[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training data: list of (review list of words, label) tuples\n",
    "trainingData = [(r.split(), 'pos') for r in posReviews] + [(r.split(), 'neg') for r in negReviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAIVE BAYES CLASSIFIER: extract feature vector from each review, train classifier\n",
    "def featureVector(reviewSplit):\n",
    "    reviewWords = set(reviewSplit)\n",
    "    features = {}\n",
    "    for word in vocabulary:\n",
    "        features[word] = word in reviewWords\n",
    "    return features\n",
    "\n",
    "naiveBayesClassifier = nltk.NaiveBayesClassifier.train(nltk.classify.apply_features(featureVector, trainingData))\n",
    "\n",
    "#classify functions\n",
    "def NBclassify(review):\n",
    "    review = removeStopwords(review)\n",
    "    features = featureVector(review.split())\n",
    "    probDist = naiveBayesClassifier.prob_classify(features)\n",
    "    confidence = max(probDist.prob('pos'), probDist.prob('neg'))\n",
    "    #pos and neg add to 1\n",
    "    return (naiveBayesClassifier.classify(features).upper(), confidence)\n",
    "\n",
    "def NBclassifyComments(comments):\n",
    "    return [(c, NBclassify(c)) for c in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBlabeledComments = NBclassifyComments(effectiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM CLASSIFIER TRAINING\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True, ngram_range=(1, 2))\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "train_vectors = vectorizer.fit_transform(posReviews + negReviews)\n",
    "labelsList = ['pos'] * len(posReviews) + ['neg'] * len(negReviews)\n",
    "\n",
    "classifier_linear = SVC(kernel='linear', probability = True)\n",
    "classifier_linear.fit(train_vectors, labelsList)\n",
    "\n",
    "classifier_poly = SVC(kernel='poly', probability = True)\n",
    "classifier_poly.fit(train_vectors, labelsList)\n",
    "\n",
    "classifier_rbf = SVC(kernel='rbf', probability = True)\n",
    "classifier_rbf.fit(train_vectors, labelsList)\n",
    "\n",
    "classifier_sigmoid = SVC(kernel='sigmoid', probability = True)\n",
    "classifier_sigmoid.fit(train_vectors, labelsList)\n",
    "\n",
    "# svm kernel can be ‘linear’, ‘poly’, ‘rbf’, or ‘sigmoid’\n",
    "def SVMclassify(review, kernel):\n",
    "    review = removeStopwords(review)\n",
    "    review_vector = vectorizer.transform([review]) # vectorizing\n",
    "    if kernel == 'linear':\n",
    "        return (classifier_linear.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    elif kernel == 'poly':\n",
    "        return (classifier_poly.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    elif kernel == 'rbf':\n",
    "        return (classifier_rbf.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    elif kernel == 'sigmoid':\n",
    "        return (classifier_sigmoid.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    return None\n",
    "\n",
    "def SVMclassifyComments(comments, kernel):\n",
    "    return [(c, SVMclassify(c, kernel)) for c in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelling new data\n",
    "SVMlabelledComments = SVMclassifyComments(comments, 'rbf')\n",
    "labelledPos = []\n",
    "labelledNeg = []\n",
    "#lc is (<comment>, (<pos/neg>, <confidence>))\n",
    "for lc in SVMlabelledComments:\n",
    "    if lc[1][0] == 'pos':\n",
    "        labelledPos.append(lc[0])\n",
    "    elif lc[1][0] == 'neg':\n",
    "        labelledNeg.append(lc[0])\n",
    "#remove stopwords\n",
    "labelledPos = list(filter(lambda s: s , list(map(removeStopwords, labelledPos))))\n",
    "labelledNeg = list(filter(lambda s: s , list(map(removeStopwords, labelledNeg))))\n",
    "\n",
    "#param [(<comment>, (<label>, <confidence>))]\n",
    "def printLabels(labelledComments):\n",
    "    print('OUTPUT OF SUPPORT VECTOR MACHINE CLASSIFIER:')\n",
    "    print(f'{len(labelledPos)} positive-labeled comments, {len(labelledNeg)} negative-labeled comments\\n\\n')\n",
    "    for lc in labelledComments:\n",
    "        print(lc[1][0].upper(), round(lc[1][1], 3), ':', f'\\\"{lc[0]}\\\"', '\\n')\n",
    "\n",
    "#printLabels(SVMlabelledComments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF CLUSTERING MODEL\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def get_topics(components, feature_names, n=50):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"\\nTopic %d: \" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "        \n",
    "v = TfidfVectorizer(max_features=1000) \n",
    "X = v.fit_transform(labelledPos)\n",
    "\n",
    "nmf_model = NMF(n_components=2, init='random', random_state=0)\n",
    "nmf_top = nmf_model.fit_transform(X)\n",
    "\n",
    "terms = v.get_feature_names() \n",
    "#get_topics(nmf_model.components_,terms)\n",
    "#nmf_model.components_\n",
    "\n",
    "for idx, topic in enumerate(nmf_model.components_):\n",
    "    if idx == 0:\n",
    "        topic_x = [(terms[i], topic[i].round(2)) for i in topic.argsort()[:-1000 - 1:-1]]\n",
    "        topic_x = {i[0]:i[1] for i in topic_x}\n",
    "#wordcloud = WordCloud(width = 3000, height = 3000, stopwords=STOPWORDS, background_color=\"white\", min_font_size = 30)\n",
    "#wordcloud = wordcloud.generate_from_frequencies(topic_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDCLOUD STUFF\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cloudStopwords = set(stopwords.words('english') + list(punctuation))\n",
    "ignore = ['resolve', 'resolved', 'resolution', 'issue', 'problem', 'solve', 'ticket', 'request']\n",
    "cloudStopwords = cloudStopwords.union(set(ignore))\n",
    "\n",
    "#calculates word frequencies for generating wordcloud\n",
    "#parameter should be list of comments, returns dictionary\n",
    "def wordFrequencies(commentList):\n",
    "    dictionary = {}\n",
    "    words = ' '.join(commentList).lower().split()\n",
    "    for w in words:\n",
    "        if w not in dictionary and w not in cloudStopwords:\n",
    "            dictionary[w] = words.count(w)\n",
    "    return dictionary\n",
    "\n",
    "def customWordFrequencies(commentList, customWords):\n",
    "    dictionary = {}\n",
    "    words = ' '.join(commentList).lower().split()\n",
    "    for w in words:\n",
    "        if w in customWords and w not in cloudStopwords:\n",
    "            dictionary[w] = words.count(w)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1e892ff0550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS/NEG WORDCLOUDS\n",
    "posCloud = WordCloud(width = 2000, height = 2000, background_color=\"white\", min_font_size = 30, max_words = 30)\n",
    "posCloud.generate_from_frequencies(wordFrequencies(labelledPos))\n",
    "\n",
    "negCloud = WordCloud(width = 2000, height = 2000, background_color=\"white\", min_font_size = 30, max_words = 30)\n",
    "negCloud.generate_from_frequencies(wordFrequencies(labelledNeg))\n",
    "#negCloud.generate(' '.join(labelledNeg).lower())\n",
    "#collocation_threshold??\n",
    "\n",
    "#posCloud.to_file(\"positive_comments_wc.png\")\n",
    "#negCloud.to_file(\"negative_comments_wc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMEANS CLUSTERING MODEL\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "\n",
    "#uses KMeans algorithm to cluster comments into numClusters groups\n",
    "#takes in comment list and number of clusters as param\n",
    "#returns dictionary of clusters (cluster number key : list of comments value) (<int> : <list(str)>)\n",
    "def clusterComments(comments, numClusters):\n",
    "    tfidf = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True, ngram_range=(1, 2))\n",
    "    x = tfidf.fit_transform(comments)\n",
    "    km = KMeans(n_clusters = numClusters, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "    km.fit(x)\n",
    "    np.unique(km.labels_, return_counts = True)\n",
    "    print(len(km.labels_))\n",
    "    dictionary = {}\n",
    "    for i,cluster in enumerate(km.labels_):\n",
    "        c = comments[i]\n",
    "        if cluster not in dictionary.keys():\n",
    "            dictionary[cluster] = [c.lower()]\n",
    "        else:\n",
    "            dictionary[cluster].append(c.lower())\n",
    "    #print(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "#takes in result of clusterComments\n",
    "#returns dictionary of <cluster : keywords>\n",
    "def findKeywords(dictionary):\n",
    "    keywords = {}\n",
    "    for cluster in range(len(dictionary)):\n",
    "        freq = wordFrequencies(dictionary[cluster])\n",
    "        keywords[cluster] = freq\n",
    "    #print(keywords)\n",
    "    return keywords\n",
    "\n",
    "#takes in result of clusterComments\n",
    "#returns dictionary of <cluster : keywords>\n",
    "def findUniqueKeywords(dictionary):\n",
    "    uniqueKeywords = {}\n",
    "    keywords = findKeywords(dictionary)\n",
    "    for cluster in range(len(dictionary)):\n",
    "        other_clusters = list(set(range(len(dictionary))) - set([cluster]))\n",
    "        keywords_other_clusters = set([])\n",
    "        for i in range(len(other_clusters)):\n",
    "            keywords_other_clusters = keywords_other_clusters.union(set(keywords[other_clusters[i]]))\n",
    "        unique = set(keywords[cluster]) - keywords_other_clusters\n",
    "        uniqueKeywords[cluster] = nlargest(100, unique, key=counts[cluster].get)\n",
    "    return uniqueKeywords\n",
    "\n",
    "#keywords is dictionary<int : [str]>\n",
    "#uniqueKeys is dictionary<int : [str]>\n",
    "#counts is dictionary<int : FreqDist>\n",
    "\n",
    "#takes in a dictionary of <cluster : keywords> as argument\n",
    "#returns nothing, just generates wordclouds for each cluster\n",
    "#=>clusterComments, findKeywords, customWordFrequencies\n",
    "def generateClusterClouds(comments, numClusters):\n",
    "    clusterCommentsDictionary = clusterComments(comments, numClusters)\n",
    "    clusterKeywordDictionary = findKeywords(clusterCommentsDictionary)\n",
    "    for cluster in clusterKeywordDictionary:\n",
    "        customCloud = WordCloud(width = 2000, height = 2000, background_color=\"white\", min_font_size = 30, max_words = 30)\n",
    "        \n",
    "        commentsOfCluster = clusterCommentsDictionary[cluster]\n",
    "        print(commentsOfCluster)\n",
    "        \n",
    "        customWords = clusterKeywordDictionary[cluster]\n",
    "        print(customWords)\n",
    "        \n",
    "        frequencies = customWordFrequencies(commentsOfCluster, customWords)\n",
    "        print(frequencies)\n",
    "        \n",
    "        customCloud.generate_from_frequencies(frequencies)\n",
    "        plt.axis(\"off\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(customCloud, interpolation=\"bilinear\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClusterClouds2(comments, numClusters):\n",
    "    clusterCommentsDictionary = clusterComments(comments, numClusters)\n",
    "    for cluster in clusterCommentsDictionary:\n",
    "        customCloud = WordCloud(width = 2000, height = 2000, background_color=\"white\", min_font_size = 30, max_words = 30)\n",
    "        \n",
    "        commentsOfCluster = clusterCommentsDictionary[cluster]\n",
    "        \n",
    "        frequencies = wordFrequencies(commentsOfCluster)\n",
    "        \n",
    "        customCloud.generate_from_frequencies(frequencies)\n",
    "        plt.axis(\"off\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(customCloud, interpolation=\"bilinear\")\n",
    "        plt.show()\n",
    "#generateClusterClouds2(labelledPos, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To Do:\\ncluster\\nfix punctuation thing\\nput as many things into functions as possible\\ntweak model and training (different vectorizer, kernel, implementing stemming, stopwords)\\nmake interface'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To Do:\n",
    "cluster\n",
    "fix punctuation thing\n",
    "put as many things into functions as possible\n",
    "tweak model and training (different vectorizer, kernel, implementing stemming, stopwords)\n",
    "make interface'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    class RunScope:\n",
    "        rawComments = [] #list of strings, each string is a comment\n",
    "        labelledComments = [] #list of strings, each string is label and associated comment\n",
    "        posLabelled = [] #list of strings, each string is a positive-labeled comment\n",
    "        negLabelled = [] #list of strings, each string is a negative-labeled comment\n",
    "    \n",
    "    def rdxl(path):\n",
    "        #C:\\Users\\n1555085\\Downloads\\May_Help_Hub_Data.xlsx\n",
    "        sheet = pandas.read_excel(path)\n",
    "        courtesy = sheet['Unnamed: 9'].dropna().values.tolist()[1:]\n",
    "        effectiveness = sheet['Unnamed: 10'].dropna().values.tolist()[1:]\n",
    "        timeliness = sheet['Unnamed: 11'].dropna().values.tolist()[1:]\n",
    "        understanding = sheet['Unnamed: 12'].dropna().values.tolist()[1:]\n",
    "        nps = sheet['Unnamed: 13'].dropna().values.tolist()[1:]\n",
    "        comments = courtesy + effectiveness + timeliness + understanding + nps\n",
    "        date = sheet['Unnamed: 1'].dropna().values.tolist()[1:]\n",
    "        #should add completion rate\n",
    "        print(f'# of courtesy comments: {len(courtesy)}')\n",
    "        print(f'# of effectiveness comments: {len(effectiveness)}')\n",
    "        print(f'# of timeliness comments: {len(timeliness)}')\n",
    "        print(f'# of understanding comments: {len(understanding)}')\n",
    "        print(f'# of nps comments: {len(nps)}')\n",
    "        print(f'# of comments: {len(comments)}')\n",
    "        RunScope.rawComments = comments\n",
    "    \n",
    "    def clsfy(comments):\n",
    "        RunScope.labelledComments = SVMclassifyComments(comments, 'rbf')\n",
    "        RunScope.posLabelled = []\n",
    "        RunScope.negLabelled = []\n",
    "        #lc is (<comment>, (<pos/neg>, <confidence>))\n",
    "        for lc in RunScope.labelledComments:\n",
    "            if lc[1][0] == 'pos':\n",
    "                RunScope.posLabelled.append(lc[0])\n",
    "            elif lc[1][0] == 'neg':\n",
    "                RunScope.negLabelled.append(lc[0])\n",
    "        #remove stopwords\n",
    "        RunScope.posLabelled = list(filter(lambda s: s , list(map(removeStopwords, RunScope.posLabelled))))\n",
    "        RunScope.negLabelled = list(filter(lambda s: s , list(map(removeStopwords, RunScope.negLabelled))))\n",
    "        printLabels(RunScope.labelledComments)\n",
    "        \n",
    "    def clsfycmt(comment):\n",
    "        classifiedComment = SVMclassify(comment, 'rbf')\n",
    "        print(f'{classifiedComment[0].upper()} {round(classifiedComment[1], 3)}')\n",
    "    \n",
    "    def wc(comments):\n",
    "        cloud = WordCloud(width = 2000, height = 2000, background_color=\"white\", min_font_size = 30, max_words = 30)\n",
    "        cloud.generate_from_frequencies(wordFrequencies(comments))\n",
    "        plt.axis(\"off\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cloud, interpolation=\"bilinear\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def kw(words):\n",
    "        freq = wordFrequencies(words)\n",
    "        #<word: frequency>\n",
    "        #for f in freq.values():\n",
    "        print(freq)\n",
    "\n",
    "    def help():\n",
    "        toPrint = \"CSAT Survey Analyzer Help:\"\n",
    "        toPrint += \"\\n    rdxl <file path> - read all comments from excel sheet\"\n",
    "        toPrint += \"\\n    clsfy - classify all comments from excel sheet\"\n",
    "        toPrint += \"\\n    clsfycmt <comment> - classify a specific comment\"\n",
    "        toPrint += \"\\n    wcp - generate wordcloud of positive-labeled comments\"\n",
    "        toPrint += \"\\n    wcn - generate wordcloud of negative-labeled comments\"\n",
    "        toPrint += \"\\n    kwp - find keywords of positive-labeled comments\"\n",
    "        toPrint += \"\\n    kwn - find keywords of negative-labeled comments\"\n",
    "        toPrint += \"\\n    help - print this help menu\"\n",
    "        toPrint += \"\\n    about - print detailed description of this program and capabilities\"\n",
    "        toPrint += \"\\n    quit - quit program\"\n",
    "        print(toPrint)\n",
    "    \n",
    "    def about():\n",
    "        toPrint = 'About CSAT Survey Analyzer'\n",
    "        #more stuff here\n",
    "        print(toPrint)\n",
    "        \n",
    "        \n",
    "    def numArgs(splitInput):\n",
    "        return len(splitInput) - 1\n",
    "    def wrongNumArgsMessage(commandName, expectedNum, numArgs):\n",
    "        return f'The command \\\"{commandName}\\\" takes in {expectedNum} argument(s), you put in {numArgs}. Please type again carefully'\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Welcome to CSAT Survey Analyzer\")\n",
    "    help(); print('\\n\\n>')\n",
    "    userInput = input()\n",
    "    while userInput != \"quit\":\n",
    "        splitInput = userInput.split()\n",
    "        if len(splitInput) > 0:\n",
    "            if splitInput[0] == \"rdxl\": #<file path>\n",
    "                if numArgs(splitInput) != 1:\n",
    "                    print(wrongNumArgsMessage(\"rdxl\", 1, numArgs(splitInput))) \n",
    "                    print('Please make sure file path has no spaces')\n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                rdxl(splitInput[1])\n",
    "            elif splitInput[0] == \"clsfy\": #no argument\n",
    "                if numArgs(splitInput) != 0:\n",
    "                    print(wrongNumArgsMessage(\"clsfy\", 0, numArgs(splitInput))) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                clsfy(RunScope.rawComments)\n",
    "                #print(RunScope.posLabelled)\n",
    "                #print(RunScope.negLabelled)\n",
    "            elif splitInput[0] == \"clsfycmt\": #<comment>, splitInput can be however long >= 2\n",
    "                if numArgs(splitInput) == 0:\n",
    "                    print(wrongNumArgsMessage(\"clsfycmt\", 1, 0)) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                clsfycmt(' '.join(splitInput[1:]))\n",
    "            elif splitInput[0] == \"wcp\": #no argument\n",
    "                if numArgs(splitInput) != 0:\n",
    "                    print(wrongNumArgsMessage(\"wcp\", 0, numArgs(splitInput))) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                wc(RunScope.posLabelled)\n",
    "            elif splitInput[0] == \"wcn\": #no argument\n",
    "                if numArgs(splitInput) != 0:\n",
    "                    print(wrongNumArgsMessage(\"wcn\", 0, numArgs(splitInput))) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                wc(RunScope.negLabelled)\n",
    "            elif splitInput[0] == \"kwp\": #no argument\n",
    "                if numArgs(splitInput) != 0:\n",
    "                    print(wrongNumArgsMessage(\"kwp\", 0, numArgs(splitInput))) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                kw(RunScope.posLabelled)\n",
    "            elif splitInput[0] == \"kwn\": #no argument\n",
    "                if numArgs(splitInput) != 0:\n",
    "                    print(wrongNumArgsMessage(\"kwn\", 0, numArgs(splitInput))) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                kw(RunScope.negLabelled) \n",
    "            elif splitInput[0] == \"help\": #no argument\n",
    "                if numArgs(splitInput) != 0:\n",
    "                    print(wrongNumArgsMessage(\"help\", 0, numArgs(splitInput))) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                help()\n",
    "            elif splitInput[0] == \"about\": #no argument\n",
    "                if numArgs(splitInput) != 0:\n",
    "                    print(wrongNumArgsMessage(\"about\", 0, numArgs(splitInput))) \n",
    "                    print(\"\\n> \")\n",
    "                    userInput = input()\n",
    "                    continue\n",
    "                about()\n",
    "            else: print(f'{splitInput[0]} is an invalid command. Please type again carefully\\n')\n",
    "        print(\"\\n\\n> \")\n",
    "        userInput = input()\n",
    "    print(\"...Program exited\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to CSAT Survey Analyzer\n",
      "CSAT Survey Analyzer Help:\n",
      "    rdxl <file path> - read all comments from excel sheet\n",
      "    clsfy - classify all comments from excel sheet\n",
      "    clsfycmt <comment> - classify a specific comment\n",
      "    wcp - generate wordcloud of positive-labeled comments\n",
      "    wcn - generate wordcloud of negative-labeled comments\n",
      "    kwp - find keywords of positive-labeled comments\n",
      "    kwn - find keywords of negative-labeled comments\n",
      "    help - print this help menu\n",
      "    about - print detailed description of this program and capabilities\n",
      "    quit - quit program\n",
      "\n",
      "\n",
      ">\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Program exited\n"
     ]
    }
   ],
   "source": [
    "run() #C:\\Users\\n1555085\\Downloads\\May_Help_Hub_Data.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(sorted(stopwords.words('english')))\n",
    "# note, might want to remove following stopwords: ['not', 'no', '!', 'but', 'too', 'have', 'had']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c8b838fcec05b01df224a370c312f42925fb843b2fc15962"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
