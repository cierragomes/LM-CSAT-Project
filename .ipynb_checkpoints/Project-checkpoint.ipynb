{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "april_hh = pd.read_excel(r'C:\\Users\\n1555085\\Downloads\\Copy of CSAT Help hub April Responses.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of courtesy comments: 16\n",
      "# of effectiveness comments: 29\n",
      "# of timeliness comments: 35\n",
      "# of understanding comments: 29\n",
      "# of nps comments: 169\n",
      "# of comments: 278\n"
     ]
    }
   ],
   "source": [
    "courtesy = april_hh['Unnamed: 9'].dropna().values.tolist()[1:]\n",
    "# convert uppercase to lower\n",
    "effectiveness = april_hh['Unnamed: 10'].dropna().values.tolist()[1:]\n",
    "timeliness = april_hh['Unnamed: 11'].dropna().values.tolist()[1:]\n",
    "understanding = april_hh['Unnamed: 12'].dropna().values.tolist()[1:]\n",
    "NPS = april_hh['Unnamed: 13'].dropna().values.tolist()[1:]\n",
    "comments = courtesy + effectiveness + timeliness + understanding + NPS\n",
    "\n",
    "print(f'# of courtesy comments: {len(courtesy)}')\n",
    "print(f'# of effectiveness comments: {len(effectiveness)}')\n",
    "print(f'# of timeliness comments: {len(timeliness)}')\n",
    "print(f'# of understanding comments: {len(understanding)}')\n",
    "print(f'# of nps comments: {len(NPS)}')\n",
    "print(f'# of comments: {len(comments)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial comment processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "customSW = set(stopwords.words('english') + list(punctuation))\n",
    "def removeStopWords(sentences):\n",
    "    toReturn = [None] * len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        withoutStopwords = [word for word in sentences[i].lower().split() if word not in customSW]\n",
    "        toReturn[i] = ' '.join(withoutStopwords)\n",
    "    return toReturn\n",
    "comments = removeStopWords(comments)\n",
    "\n",
    "\n",
    "\n",
    "#customSW\n",
    "#comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already cleared cookies cache site still accessible. presumptuous answer.\n",
      "  (0, 14)\t0.519793378442015\n",
      "  (0, 207)\t0.4060783032559657\n",
      "  (0, 198)\t0.5428905037670378\n",
      "  (0, 10)\t0.519793378442015\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 3)\n",
    "X = vectorizer.fit_transform(comments)\n",
    "X\n",
    "print(comments[0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 261.55197729780446\n",
      "Iteration 1, inertia 249.10940527103858\n",
      "Iteration 2, inertia 247.50397327502418\n",
      "Iteration 3, inertia 246.67296581245463\n",
      "Iteration 4, inertia 246.38034421433557\n",
      "Iteration 5, inertia 246.3257198548414\n",
      "Converged at iteration 5: strict convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=5, n_init=1, verbose=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['resolved',\n",
       "  'ticket',\n",
       "  'say',\n",
       "  '.',\n",
       "  'emailed',\n",
       "  'made',\n",
       "  'updates',\n",
       "  'responded',\n",
       "  'wasnt',\n",
       "  'received'],\n",
       " 1: ['fixed',\n",
       "  ',',\n",
       "  'problem',\n",
       "  'resolution',\n",
       "  '.',\n",
       "  'nothing',\n",
       "  'ongoing',\n",
       "  'courteous',\n",
       "  '!',\n",
       "  'issue'],\n",
       " 2: ['.',\n",
       "  ',',\n",
       "  'issue',\n",
       "  'ticket',\n",
       "  'resolved',\n",
       "  'problem',\n",
       "  'time',\n",
       "  'help',\n",
       "  'get',\n",
       "  '!'],\n",
       " 3: ['issue',\n",
       "  'work',\n",
       "  'resolve',\n",
       "  'rather',\n",
       "  'ended',\n",
       "  'providing',\n",
       "  'around.built',\n",
       "  'around',\n",
       "  'addressed'],\n",
       " 4: ['professional',\n",
       "  ',',\n",
       "  '.',\n",
       "  'courteous',\n",
       "  'support',\n",
       "  'representatives',\n",
       "  'times.personnel',\n",
       "  'professionaltimely',\n",
       "  'professionalrichard',\n",
       "  'tedesco']}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(km.labels_, return_counts = True)\n",
    "text = {}\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    comment = comments[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = comment\n",
    "    else: \n",
    "        text[cluster] += comment\n",
    "keywords = {}\n",
    "counts = {}\n",
    "for cluster in range(5):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    frq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(10, frq, key=frq.get)\n",
    "    counts[cluster] = frq\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
