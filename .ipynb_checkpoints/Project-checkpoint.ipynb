{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    print('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n"
     ]
    }
   ],
   "source": [
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "#pandas.read_excel(r'C:\\Users\\n1555085\\Downloads\\Copy of CSAT Help hub April Responses.xlsx')\n",
    "sheet = pandas.read_excel(r'C:\\Users\\n1555085\\Downloads\\May Help Hub Data.xlsx')\n",
    "courtesy = sheet['Unnamed: 9'].dropna().values.tolist()[1:]\n",
    "effectiveness = sheet['Unnamed: 10'].dropna().values.tolist()[1:]\n",
    "timeliness = sheet['Unnamed: 11'].dropna().values.tolist()[1:]\n",
    "understanding = sheet['Unnamed: 12'].dropna().values.tolist()[1:]\n",
    "nps = sheet['Unnamed: 13'].dropna().values.tolist()[1:]\n",
    "comments = courtesy + effectiveness + timeliness + understanding + nps\n",
    "date = sheet['Unnamed: 1'].dropna().values.tolist()[1:]\n",
    "#should add completion rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of courtesy comments: 10\n",
      "# of effectiveness comments: 17\n",
      "# of timeliness comments: 24\n",
      "# of understanding comments: 13\n",
      "# of nps comments: 148\n",
      "# of comments: 212\n"
     ]
    }
   ],
   "source": [
    "print(f'# of courtesy comments: {len(courtesy)}')\n",
    "print(f'# of effectiveness comments: {len(effectiveness)}')\n",
    "print(f'# of timeliness comments: {len(timeliness)}')\n",
    "print(f'# of understanding comments: {len(understanding)}')\n",
    "print(f'# of nps comments: {len(nps)}')\n",
    "print(f'# of comments: {len(comments)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4553 1574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Spenser Cameron set assisted issues. exceptional trying assist newcomer new systems.',\n",
       " 'second consultant awesome appropriate follow well letting know break fix consultants suggestion since could not receive emails. best.',\n",
       " 'support personnel helpful patient issue',\n",
       " 'fast service.',\n",
       " 'Greeting robotic, however loosened conversation went on.']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read from corpus, remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "with open(r'C:\\Users\\n1555085\\Downloads\\Project\\positiveComments.txt', 'r') as f:\n",
    "    posReviews = f.readlines()\n",
    "with open(r'C:\\Users\\n1555085\\Downloads\\Project\\negativeComments.txt', 'r') as f:\n",
    "    negReviews = f.readlines()\n",
    "print(len(posReviews), len(negReviews))\n",
    "\n",
    "sw = set(stopwords.words('english') + list(punctuation))\n",
    "notStopwords = ['not', 'no', '!', 'but', 'too', 'have', 'had']\n",
    "\n",
    "def removeStopwords(review):\n",
    "    #review.translate(None, string.punctuation)\n",
    "    return ' '.join([word for word in review.split() if word.lower() not in sw or word.lower() in notStopwords])\n",
    "posReviews = list(filter(lambda s: s , list(map(removeStopwords, posReviews))))\n",
    "negReviews = list(filter(lambda s: s , list(map(removeStopwords, negReviews))))\n",
    "\n",
    "posReviews[0:5]\n",
    "#negReviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define pos and neg bag-of words, and vocabulary\n",
    "\n",
    "posWords = [word.lower() for review in posReviews for word in review.split()]\n",
    "negWords = [word.lower() for review in negReviews for word in review.split()]\n",
    "vocabulary = list(set(posWords + negWords))\n",
    "#vocabulary[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training data: list of (review list of words, label) tuples\n",
    "trainingData = [(r.split(), 'pos') for r in posReviews] + [(r.split(), 'neg') for r in negReviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAIVE BAYES CLASSIFIER: extract feature vector from each review, train classifier\n",
    "def featureVector(reviewSplit):\n",
    "    reviewWords = set(reviewSplit)\n",
    "    features = {}\n",
    "    for word in vocabulary:\n",
    "        features[word] = word in reviewWords\n",
    "    return features\n",
    "\n",
    "naiveBayesClassifier = nltk.NaiveBayesClassifier.train(nltk.classify.apply_features(featureVector, trainingData))\n",
    "\n",
    "#classify functions\n",
    "def NBclassify(review):\n",
    "    review = removeStopwords(review)\n",
    "    features = featureVector(review.split())\n",
    "    probDist = naiveBayesClassifier.prob_classify(features)\n",
    "    confidence = max(probDist.prob('pos'), probDist.prob('neg'))\n",
    "    #pos and neg add to 1\n",
    "    return (naiveBayesClassifier.classify(features).upper(), confidence)\n",
    "\n",
    "def NBclassifyComments(comments):\n",
    "    return [(c, NBclassify(c)) for c in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBlabeledComments = NBclassifyComments(effectiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True, ngram_range=(1, 2))\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "train_vectors = vectorizer.fit_transform(posReviews + negReviews)\n",
    "labelsList = ['pos'] * len(posReviews) + ['neg'] * len(negReviews)\n",
    "\n",
    "classifier_linear = SVC(kernel='linear', probability = True)\n",
    "classifier_linear.fit(train_vectors, labelsList)\n",
    "\n",
    "classifier_poly = SVC(kernel='poly', probability = True)\n",
    "classifier_poly.fit(train_vectors, labelsList)\n",
    "\n",
    "classifier_rbf = SVC(kernel='rbf', probability = True)\n",
    "classifier_rbf.fit(train_vectors, labelsList)\n",
    "\n",
    "classifier_sigmoid = SVC(kernel='sigmoid', probability = True)\n",
    "classifier_sigmoid.fit(train_vectors, labelsList)\n",
    "\n",
    "# svm kernel can be ‘linear’, ‘poly’, ‘rbf’, or ‘sigmoid’\n",
    "def SVMclassify(review, kernel):\n",
    "    review = removeStopwords(review)\n",
    "    review_vector = vectorizer.transform([review]) # vectorizing\n",
    "    if kernel == 'linear':\n",
    "        return (classifier_linear.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    elif kernel == 'poly':\n",
    "        return (classifier_poly.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    elif kernel == 'rbf':\n",
    "        return (classifier_rbf.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    elif kernel == 'sigmoid':\n",
    "        return (classifier_sigmoid.predict(review_vector)[0], max(classifier_rbf.predict_proba(review_vector)[0]))\n",
    "    return None\n",
    "\n",
    "def SVMclassifyComments(comments, kernel):\n",
    "    return [(c, SVMclassify(c, kernel)) for c in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelling new data\n",
    "SVMlabelledComments = SVMclassifyComments(comments, 'rbf')\n",
    "labelledPos = []\n",
    "labelledNeg = []\n",
    "#lc is (<comment>, (<pos/neg>, <confidence>))\n",
    "for lc in SVMlabelledComments:\n",
    "    if lc[1][0] == 'pos':\n",
    "        labelledPos.append(lc[0])\n",
    "    elif lc[1][0] == 'neg':\n",
    "        labelledNeg.append(lc[0])\n",
    "#remove stopwords\n",
    "labelledPos = list(filter(lambda s: s , list(map(removeStopwords, labelledPos))))\n",
    "labelledNeg = list(filter(lambda s: s , list(map(removeStopwords, labelledNeg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 275.21408166494314\n",
      "Iteration 1, inertia 130.30789571082016\n",
      "Iteration 2, inertia 130.2186164990933\n",
      "Converged at iteration 2: strict convergence.\n"
     ]
    }
   ],
   "source": [
    "#Clustering labelled data\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True, ngram_range=(1, 2))\n",
    "x = tfidf.fit_transform(labelledPos)\n",
    "km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "km.fit(x)\n",
    "np.unique(km.labels_, return_counts = True)\n",
    "text = {}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    posComment = labelledPos[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = posComment\n",
    "    else:\n",
    "        text[cluster] += posComment\n",
    "\n",
    "        \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "\n",
    "keywords = {}\n",
    "counts = {}\n",
    "for cluster in range(3):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(10, freq, key=freq.get)\n",
    "    counts[cluster] = freq\n",
    "\n",
    "uniqueKeys = {}\n",
    "for cluster in range(3):\n",
    "    other_clusters = list(set(range(3)) - set([cluster]))\n",
    "    keys_other_clusters = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique = set(keywords[cluster]) - keys_other_clusters\n",
    "    uniqueKeys[cluster] = nlargest(10, unique, key=counts[cluster].get)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:  [('quick', 1.68), ('response', 1.17), ('efficient', 1.07), ('resolution', 0.9), ('issue', 0.72), ('resolved', 0.4), ('solution', 0.35), ('fix', 0.31), ('help', 0.29), ('easy', 0.23), ('friendly', 0.21), ('clear', 0.21), ('follow', 0.2), ('solved', 0.19), ('fixed', 0.19), ('problem', 0.18), ('great', 0.18), ('rapid', 0.18), ('expertise', 0.18), ('via', 0.17), ('thanks', 0.16), ('had', 0.16), ('quickly', 0.16), ('timely', 0.14), ('pleasant', 0.14), ('service', 0.14), ('etc', 0.14), ('would', 0.13), ('time', 0.13), ('helpful', 0.13), ('support', 0.13), ('but', 0.12), ('email', 0.12), ('call', 0.12), ('assistance', 0.12), ('took', 0.11), ('nice', 0.11), ('first', 0.11), ('ticket', 0.11), ('swift', 0.1), ('contacted', 0.1), ('quicker', 0.1), ('knew', 0.1), ('fast', 0.1), ('without', 0.09), ('provided', 0.09), ('understood', 0.09), ('understand', 0.09), ('proactive', 0.09), ('completely', 0.09)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'from wordcloud import WordCloud\\nfrom wordcloud import STOPWORDS\\nimport matplotlib.pyplot as plt\\n\\nfor idx, topic in enumerate(nmf_model.components_):\\n    if idx == 0:\\n        topic_x = [(terms[i], topic[i].round(2)) for i in topic.argsort()[:-1000 - 1:-1]]\\n        topic_x = {i[0]:i[1] for i in topic_x}\\n            \\nwordcloud = WordCloud(width = 3000, height = 3000, stopwords=STOPWORDS, background_color=\"white\", min_font_size = 30)\\nwordcloud = wordcloud.generate_from_frequencies(topic_x)\\n\\nplt.axis(\"off\")\\nplt.figure(figsize=(50, 50))\\nplt.imshow(wordcloud, interpolation=\"bilinear\")\\nplt.show()'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def get_topics(components, feature_names, n=50):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"\\nTopic %d: \" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "        \n",
    "v = TfidfVectorizer(max_features=1000) \n",
    "X = v.fit_transform(labelledPos)\n",
    "\n",
    "nmf_model = NMF(n_components=1, init='random', random_state=0)\n",
    "nmf_top = nmf_model.fit_transform(X)\n",
    "\n",
    "terms = v.get_feature_names() \n",
    "get_topics(nmf_model.components_,terms)\n",
    "\n",
    "\n",
    "'''from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for idx, topic in enumerate(nmf_model.components_):\n",
    "    if idx == 0:\n",
    "        topic_x = [(terms[i], topic[i].round(2)) for i in topic.argsort()[:-1000 - 1:-1]]\n",
    "        topic_x = {i[0]:i[1] for i in topic_x}\n",
    "            \n",
    "wordcloud = WordCloud(width = 3000, height = 3000, stopwords=STOPWORDS, background_color=\"white\", min_font_size = 30)\n",
    "wordcloud = wordcloud.generate_from_frequencies(topic_x)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
